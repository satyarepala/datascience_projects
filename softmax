import numpy as np

# Example logits of shape (12, 2) for 12 samples and 2 classes (binary classification)
logits = np.array([
    [1.0, 2.0],
    [2.5, 0.5],
    [0.5, -1.0],
    [3.0, 2.0],
    [0.0, 1.0],
    [1.5, 1.5],
    [0.0, -0.5],
    [1.0, 0.5],
    [2.0, 3.0],
    [1.0, -1.0],
    [-0.5, -1.0],
    [2.0, 2.0]
])

# Softmax function to apply to logits
def softmax(logits):
    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # Numerical stability trick
    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)

# Apply softmax to get probabilities for each class (shape: 12 x 2)
probabilities = softmax(logits)

# Print the probabilities for each sample
print("Probabilities for each class:")
print(probabilities)
