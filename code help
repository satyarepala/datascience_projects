from pyspark.sql import SparkSession, functions as F, Window

# Create Spark session
spark = SparkSession.builder.appName("OutlierDetection").getOrCreate()

# Sample DataFrame (Replace with actual data)
df = spark.createDataFrame([
    (1, 10), (1, 15), (1, 12), (1, 11), (1, 14), (1, 300), (1, 9),
    (2, 200), (2, 210), (2, 215), (2, 220), (2, 225), (2, 500)
], ["cat_group_id", "BILLING_QTY"])

# Step 1: Assign Row Number
window_spec = Window.partitionBy("cat_group_id").orderBy("BILLING_QTY")
df_ranked = df.withColumn("seq", F.row_number().over(window_spec))

# Step 2: Get Max Sequence Number & Compute Q1, Q3 Index
df_max_seq = df_ranked.groupBy("cat_group_id").agg(F.max("seq").alias("max_seq"))
df_quantile_indices = df_max_seq.withColumn("q1_seq", (0.25 * F.col("max_seq")).cast("int")) \
                                .withColumn("q3_seq", (0.75 * F.col("max_seq")).cast("int"))

# Step 3: Extract Q1 & Q3 Values
df_quantiles = df_ranked.join(df_quantile_indices, "cat_group_id", "inner") \
    .withColumn("Q1", F.when(F.col("seq") == F.col("q1_seq"), F.col("BILLING_QTY"))) \
    .withColumn("Q3", F.when(F.col("seq") == F.col("q3_seq"), F.col("BILLING_QTY"))) \
    .groupBy("cat_group_id").agg(F.max("Q1").alias("Q1"), F.max("Q3").alias("Q3"))

# Step 4: Compute IQR & Outlier Boundaries
df_outlier_bounds = df_quantiles.withColumn("IQR", F.col("Q3") - F.col("Q1")) \
    .withColumn("lower_bound", F.col("Q1") - 1.5 * F.col("IQR")) \
    .withColumn("upper_bound", F.col("Q3") + 1.5 * F.col("IQR"))

# Step 5: Identify Outliers
df_final = df.join(df_outlier_bounds, "cat_group_id", "inner") \
    .withColumn("is_outlier", 
                F.when((F.col("BILLING_QTY") < F.col("lower_bound")) | 
                       (F.col("BILLING_QTY") > F.col("upper_bound")), 1).otherwise(0))

# Show results
df_final.show()