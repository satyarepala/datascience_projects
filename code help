from pyspark.sql import SparkSession
from pyspark.sql.functions import col, countDistinct, sum as spark_sum, row_number
from pyspark.sql.window import Window

# Initialize Spark Session
spark = SparkSession.builder.appName("BillingDataProcessing").getOrCreate()

# Sample Data
data = [
    ("A1", 2024, 1, 100), ("A1", 2024, 2, 200), ("A1", 2024, 3, 150), 
    ("A1", 2024, 4, 100), ("A1", 2024, 5, 120), ("A1", 2024, 6, 130),
    ("A1", 2024, 7, 140), ("A1", 2024, 8, 150), ("A1", 2024, 9, 160),
    ("A1", 2023, 1, 110), ("A1", 2023, 2, 210), ("A1", 2023, 3, 160),
    ("B1", 2024, 1, 200), ("B1", 2024, 2, 300), ("B1", 2024, 3, 250),
    ("B1", 2024, 4, 200), ("B1", 2024, 5, 220), ("B1", 2024, 6, 230),
    ("B1", 2024, 7, 240), ("B1", 2024, 8, 250), ("B1", 2024, 9, 260),
    ("B1", 2023, 1, 210), ("B1", 2023, 2, 310), ("B1", 2023, 3, 260),
]

# Create DataFrame
columns = ["material_id", "billing_year", "billing_month", "billing_quantity"]
df = spark.createDataFrame(data, columns)

# Step 1: Aggregate Data
year_summary = (
    df.groupBy("material_id", "billing_year")
    .agg(
        countDistinct("billing_month").alias("months_count"),
        spark_sum("billing_quantity").alias("total_billing_qty")
    )
)

# Step 2: Filter for at least 9 months of data
filtered_years = year_summary.filter(col("months_count") >= 9)

# Step 3: Rank years for each material_id (latest year first)
window_spec = Window.partitionBy("material_id").orderBy(col("billing_year").desc())
ranked_years = filtered_years.withColumn("year_rank", row_number().over(window_spec))

# Step 4: Select the best year per material_id
final_data = ranked_years.filter(col("year_rank") == 1).select("material_id", "billing_year", "total_billing_qty")

# Show result
final_data.show()