from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Create Spark session
spark = SparkSession.builder.appName("FillMissingMonths").getOrCreate()

# Sample Data (Replace with actual DataFrame)
data = [
    (1, "A", 1, 100), (1, "A", 2, 150), (1, "A", 4, 130), # Missing months for material A
    (1, "B", 1, 200), (1, "B", 2, 250), (1, "B", 3, 230), (1, "B", 4, 240) # Full months for B
]
columns = ["cat_group_id", "material_id", "billing_month", "billing_qty"]

df = spark.createDataFrame(data, columns)

# Step 1: Generate all months (1-12) for each material_id
months_df = spark.createDataFrame([(i,) for i in range(1, 13)], ["billing_month"])
material_months = df.select("cat_group_id", "material_id").distinct().crossJoin(months_df)

# Step 2: Outer join to ensure all months exist
df_filled = material_months.join(df, ["cat_group_id", "material_id", "billing_month"], "left")

# Step 3: Compute mean billing_qty for each material_id
window_spec = Window.partitionBy("material_id")
df_filled = df_filled.withColumn(
    "billing_qty",
    F.when(F.col("billing_qty").isNull(), F.avg("billing_qty").over(window_spec)).otherwise(F.col("billing_qty"))
)

# Show the final DataFrame
df_filled.orderBy("material_id", "billing_month").show()