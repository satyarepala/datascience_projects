import torch
import torch.nn as nn
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
import matplotlib.pyplot as plt
import seaborn as sns
import shap
from captum.attr import IntegratedGradients
import numpy as np

# Define a custom TransformerEncoderLayer to capture attention weights
class CustomTransformerEncoderLayer(nn.TransformerEncoderLayer):
    def __init__(self, *args, **kwargs):
        super(CustomTransformerEncoderLayer, self).__init__(*args, **kwargs)
        self.attention_weights = None

    def forward(self, src, *args, **kwargs):
        src2, attn_weights = self.self_attn(src, src, src, need_weights=True)
        self.attention_weights = attn_weights  # Save attention weights
        src = src + self.dropout1(src2)
        src = self.norm1(src)
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
        src = src + self.dropout2(src2)
        src = self.norm2(src)
        return src

# Update the TimeSeriesTransformer class
class TimeSeriesTransformer(nn.Module):
    def __init__(self, input_dim, model_dim, num_heads, num_layers, dropout=0.1):
        super(TimeSeriesTransformer, self).__init__()

        self.embedding = nn.Linear(input_dim, model_dim)

        self.encoder_layers = nn.ModuleList([
            CustomTransformerEncoderLayer(
                d_model=model_dim,
                nhead=num_heads,
                dim_feedforward=model_dim * 4,
                dropout=dropout
            ) for _ in range(num_layers)
        ])

        self.fc = nn.Linear(model_dim, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x, lengths):
        x = self.embedding(x)

        # Pack the sequence
        packed_input = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)

        # Unpack the packed input to pass through encoder layers
        unpacked_input, _ = pad_packed_sequence(packed_input, batch_first=False)

        # Process through encoder layers and collect attention weights
        attention_weights = []
        encoded_output = unpacked_input

        for layer in self.encoder_layers:
            encoded_output = layer(encoded_output)
            attention_weights.append(layer.attention_weights)  # Save attention weights

        # Repack the encoded output
        packed_output = pack_padded_sequence(encoded_output, lengths, batch_first=False, enforce_sorted=False)
        unpacked_output, _ = pad_packed_sequence(packed_output, batch_first=True)

        # Extract the outputs corresponding to the last valid time step for each sequence
        output = torch.stack([unpacked_output[i, l-1] for i, l in enumerate(lengths)], dim=0)
        output = self.fc(output)

        return self.sigmoid(output), attention_weights

# Initialize the model
input_dim = 32
model_dim = 64
num_heads = 4
num_layers = 2
model = TimeSeriesTransformer(input_dim, model_dim, num_heads, num_layers)

# Create a sample input for testing
batch_size = 3
seq_len = 100
sample_input = torch.randn(batch_size, seq_len, input_dim)
lengths = torch.tensor([100, 80, 60])  # Example lengths for each sequence in the batch

# Test the model
model.eval()
with torch.no_grad():
    outputs, attention_weights = model(sample_input, lengths)
outputs

def overlay_attention_on_dataframe_with_values(df, attention_weights, sequence_index, layer_index=0):
    """
    Visualizes a Pandas DataFrame with feature values as text and overlays attention weights as a heatmap.

    Parameters:
        df (pd.DataFrame): Input DataFrame (rows: time steps, cols: features).
        attention_weights (list): List of attention weights from all layers.
        sequence_index (int): Index of the sequence in the batch to visualize.
        layer_index (int): Layer index to extract attention weights from.
    """
    # Extract attention weights for the specific layer
    attn = attention_weights[layer_index].detach().cpu().numpy()  # Shape: (num_heads, seq_len, seq_len)

    # Get the attention weights for the specific sequence
    attn_for_sequence = attn[:, sequence_index, :]  # Shape: (num_heads, seq_len)

    # Average across the heads
    attn_mean = attn_for_sequence.mean(axis=0)  # Shape: (seq_len,)

    # Normalize the attention weights to range [0, 1]
    attn_normalized = (attn_mean - attn_mean.min()) / (attn_mean.max() - attn_mean.min())

    # Create a mask of attention values for overlaying
    overlay = np.outer(attn_normalized, np.ones(df.shape[1]))  # Shape: (seq_len, num_features)

    # Ensure annotations are strings
    annotations = df.astype(str).values

    # Plot the DataFrame with attention overlay
    plt.figure(figsize=(24, 16))
    sns.heatmap(
        overlay,
        cmap="Reds",
        alpha=0.5,
        cbar=True,
        xticklabels=df.columns,
        yticklabels=df.index,
        annot=annotations,  # Use string annotations
        fmt="",  # Avoid formatting issues
        annot_kws={"size": 8, "color": "black"},
        linewidths=0.5,
    )
    plt.title(f"Input Features with Attention Overlay (Layer {layer_index + 1})", fontsize=16)
    plt.xlabel("Features", fontsize=14)
    plt.ylabel("Time Steps", fontsize=14)
    plt.show()


# Generate a sample Pandas DataFrame (100 rows, 32 columns)
np.random.seed(42)
sample_df = pd.DataFrame(
    np.random.choice([42, "hello", "world", np.random.randn()], size=(100, 32)),
    columns=[f"Feature_{i+1}" for i in range(32)],
)

# Overlay attention weights on the DataFrame for the first sequence and first layer
overlay_attention_on_dataframe_with_values(sample_df, attention_weights, sequence_index=0, layer_index=0)
