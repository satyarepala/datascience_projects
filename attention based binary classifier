 import torch
import torch.nn as nn
from torch.nn import MultiheadAttention

class AttentionClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_heads):
        super(AttentionClassifier, self).__init__()
        
        # Project input to a size divisible by num_heads (e.g., 128)
        projected_size = 128  # Change to a divisible size
        self.input_projection = nn.Linear(input_size, projected_size)
        
        # Multi-Head Attention layer
        self.attn = MultiheadAttention(embed_dim=projected_size, num_heads=num_heads, batch_first=True)
        
        # Positional encoding
        self.position_embedding = nn.Embedding(100, projected_size)  # Assuming 100 time steps
        
        # Fully connected classification layer
        self.fc = nn.Linear(projected_size, 1)  # Binary classification
    
    def forward(self, x, lengths):
        # Project input to the required size
        x = self.input_projection(x)
        
        # Add positional encoding
        position = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)
        position_embedding = self.position_embedding(position)
        x = x + position_embedding
        
        # Generate attention mask
        attn_mask = self._generate_attention_mask(lengths, x.size(1), x.device)
        
        # Apply attention
        attn_output, attn_weights = self.attn(x, x, x, key_padding_mask=~attn_mask)
        
        # Use the output of the last time step
        attn_output = attn_output[:, -1, :]
        
        # Classification layer
        output = self.fc(attn_output)
        
        return output

    def _generate_attention_mask(self, lengths, max_len, device):
        mask = torch.arange(max_len, device=device).unsqueeze(0) < lengths.unsqueeze(1)
        return mask

# Example usage
input_size = 101  # Input size is 101
hidden_size = 128
num_heads = 4  # Choose a divisor of 128
num_layers = 1

model = AttentionClassifier(input_size, hidden_size, num_layers, num_heads)

x = torch.randn(32, 100, input_size)  # 32 samples, 100 timesteps, 101 features
lengths = torch.randint(1, 100, (32,))
output = model(x, lengths)

print(output.shape)  # Output: (32, 1)