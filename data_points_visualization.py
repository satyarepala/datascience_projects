# -*- coding: utf-8 -*-
"""data_points visualization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nKVzqSNnU8rgx2TMt47ojsx6Pa2NNU6y
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# Example lists with sublists (datapoints)
list1 = np.random.rand(500, 200)  # 500 sublists, each of length 200
list2 = np.random.rand(2500, 200)  # 2500 sublists, each of length 200
print(list1.shape), print(list2.shape)
# Combine the lists
data = np.concatenate((list1, list2), axis=0)

# Apply PCA to reduce to 2 dimensions
pca = PCA(n_components=2)
data_reduced_pca = pca.fit_transform(data)

# Apply t-SNE to reduce to 2 dimensions
tsne = TSNE(n_components=2, random_state=42)
data_reduced_tsne = tsne.fit_transform(data)

# Create color labels for visualization
colors = ['red'] * len(list1) + ['blue'] * len(list2)

# Plot PCA result
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.scatter(data_reduced_pca[:, 0], data_reduced_pca[:, 1], c=colors, alpha=0.6)
plt.title('PCA')

# Plot t-SNE result
plt.subplot(1, 2, 2)
plt.scatter(data_reduced_tsne[:, 0], data_reduced_tsne[:, 1], c=colors, alpha=0.6)
plt.title('t-SNE')

plt.show()



from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Example data
list1 = np.random.rand(500, 200)  # 500 sublists, each of length 200
list2 = np.random.rand(2500, 200)  # 2500 sublists, each of length 200
data = np.concatenate((list1, list2), axis=0)
colors = ['red'] * len(list1) + ['blue'] * len(list2)

# Apply PCA to reduce to 3 dimensions
pca = PCA(n_components=3)
data_reduced_pca = pca.fit_transform(data)

# Plot in 3D
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(data_reduced_pca[:, 0], data_reduced_pca[:, 1], data_reduced_pca[:, 2], c=colors, alpha=0.6)
ax.set_title('PCA 3D Visualization')
plt.show()








import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score

# Step 1: Generate Sample Data
np.random.seed(42)

# Class 1 and Class 2 data (500 points each, 200 features)
class1 = np.random.rand(500, 200) + np.array([0.5] * 200)
class2 = np.random.rand(500, 200) + np.array([-0.5] * 200)

# Labels
labels_class1 = np.zeros(500)  # Label 0 for Class 1
labels_class2 = np.ones(500)   # Label 1 for Class 2

# Combine data and labels
X = np.vstack((class1, class2))
y = np.hstack((labels_class1, labels_class2))

# Step 2: Split the Data (80% training, 20% testing) with stratification
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Step 3: Train SVM Model with RBF Kernel
svm_model = SVC(kernel='rbf', C=1.0, gamma='scale')  # C and gamma are hyperparameters to tune
svm_model.fit(X_train, y_train)

# Step 4: Predict on Test Data
y_pred = svm_model.predict(X_test)

# Step 5: Evaluate the Model
# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

# Classification Report (includes precision, recall, F1-score)
class_report = classification_report(y_test, y_pred, target_names=['Class 1', 'Class 2'])
print("\nClassification Report:")
print(class_report)

# Precision and Recall Scores
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

print(f"\nPrecision: {precision:.2f}")
print(f"Recall: {recall:.2f}")



