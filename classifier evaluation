from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score

# Assuming you have your labels and predictions in these lists:
# Replace 'labels' and 'preds' with your actual data
labels = [...]  # true labels
preds = [...]   # predicted labels or probabilities for binary classification

# Confusion matrix to get TP, FP, TN, FN
tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()

# Calculate the metrics
accuracy = accuracy_score(labels, preds)
precision = precision_score(labels, preds)
recall = recall_score(labels, preds)
f1 = f1_score(labels, preds)

# For AUC-ROC, if `preds` are probabilities, pass those directly; if they are binary, it's okay to use them as is
roc_auc = roc_auc_score(labels, preds)

# PRC-AUC (Precision-Recall Curve AUC)
prc_auc = average_precision_score(labels, preds)

# Print all the metrics
print(f"True Positives (TP): {tp}")
print(f"False Positives (FP): {fp}")
print(f"True Negatives (TN): {tn}")
print(f"False Negatives (FN): {fn}")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"ROC AUC: {roc_auc:.4f}")
print(f"PRC AUC: {prc_auc:.4f}")