import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import precision_score, recall_score, roc_auc_score, average_precision_score, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
from torch.optim import Adam
from torch.nn.functional import binary_cross_entropy_with_logits

# Sample Data
class1_samples = 100
class2_samples = 1000

# Generate random sample data: 100 timesteps, 10 features
X_class1 = np.random.randn(class1_samples, 100, 10)
X_class2 = np.random.randn(class2_samples, 100, 10)

# Create labels: Class 1 = 1, Class 2 = 0
y_class1 = np.ones((class1_samples, 1))
y_class2 = np.zeros((class2_samples, 1))

# Combine class data
X = np.vstack((X_class1, X_class2))
y = np.vstack((y_class1, y_class2))

# Convert to tensors
X_tensor = torch.tensor(X, dtype=torch.float32)
y_tensor = torch.tensor(y, dtype=torch.float32)

# Create a DataLoader
dataset = TensorDataset(X_tensor, y_tensor)
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# LSTM Binary Classifier
class LSTMClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers):
        super(LSTMClassifier, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)
    
    def forward(self, x):
        h0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)
        c0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])  # Get the output of the last timestep
        return out

# Initialize model, loss function, and optimizer
input_size = 10
hidden_size = 64
num_layers = 2
model = LSTMClassifier(input_size, hidden_size, num_layers)

# Calculate class weights
class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=y.flatten())
class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)

# Optimizer and Loss function
optimizer = Adam(model.parameters(), lr=0.001)

# Early stopping params
patience = 2
best_loss = float('inf')
counter = 0

# Training function

for epoch in range(5):
    model.train()
    train_loss = 0.0
    all_preds = []
    all_labels = []
    
    # Training loop
    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        outputs = model(X_batch).squeeze()
        loss = binary_cross_entropy_with_logits(outputs, y_batch.squeeze(), pos_weight=class_weights_tensor[1])
        loss.backward()
        optimizer.step()
        
        train_loss += loss.item()
        preds = torch.sigmoid(outputs).detach().cpu().numpy() > 0.5
        all_preds.extend(preds)
        all_labels.extend(y_batch.cpu().numpy())
    
    # Validation loop
    model.eval()
    val_loss = 0.0
    val_preds = []
    val_labels = []
    with torch.no_grad():
        for X_val_batch, y_val_batch in val_loader:
            outputs = model(X_val_batch).squeeze()
            val_loss += binary_cross_entropy_with_logits(outputs, y_val_batch.squeeze(), pos_weight=class_weights_tensor[1]).item()
            preds = torch.sigmoid(outputs).detach().cpu().numpy() > 0.5
            val_preds.extend(preds)
            val_labels.extend(y_val_batch.cpu().numpy())
    
    # Compute metrics
    train_precision = precision_score(all_labels, all_preds)
    train_recall = recall_score(all_labels, all_preds)
    train_roc_auc = roc_auc_score(all_labels, all_preds)
    train_prc_auc = average_precision_score(all_labels, all_preds)
    
    val_precision = precision_score(val_labels, val_preds)
    val_recall = recall_score(val_labels, val_preds)
    val_roc_auc = roc_auc_score(val_labels, val_preds)
    val_prc_auc = average_precision_score(val_labels, val_preds)
    
    print(f"Epoch {epoch+1}/{20}")
    print(f"Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}")
    print(f"Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, Train PRC AUC: {train_prc_auc:.4f}, Train ROC AUC: {train_roc_auc:.4f}")
    print(f"Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}, Val PRC AUC: {val_prc_auc:.4f}, Val ROC AUC: {val_roc_auc:.4f}")
    
    # Early stopping
    if val_loss < best_loss:
        best_loss = val_loss
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping!")
            break

# Train the model
#train_model(model, train_loader, val_loader, class_weights_tensor, patience)
