import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score
import numpy as np

# Example: True labels and predicted probabilities (adjust these with your actual data)
y_true = np.array([0, 1, 0, 1, 0, 1, 0, 1])  # True labels
y_pred = np.array([0.1, 0.9, 0.2, 0.8, 0.3, 0.7, 0.4, 0.9])  # Predicted probabilities

# 1. Compute ROC curve and ROC AUC
fpr, tpr, _ = roc_curve(y_true, y_pred)
roc_auc = auc(fpr, tpr)

# 2. Compute Precision-Recall curve and PR AUC
precision, recall, _ = precision_recall_curve(y_true, y_pred)
pr_auc = average_precision_score(y_true, y_pred)

# 3. Plot ROC curve
plt.figure(figsize=(12, 6))

# Plot ROC curve
plt.subplot(1, 2, 1)
plt.plot(fpr, tpr, color='b', label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')

# 4. Plot Precision-Recall curve
plt.subplot(1, 2, 2)
plt.plot(recall, precision, color='b', label=f'PR curve (area = {pr_auc:.2f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc='lower left')

plt.tight_layout()
plt.show()

# 5. Display FPR, TPR, Precision, Recall
print("FPR:", fpr)
print("TPR:", tpr)
print("Precision:", precision)
print("Recall:", recall)