import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, random_split
import numpy as np
import matplotlib.pyplot as plt
import pickle

# Sample user-agent strings (example)
user_agents = [
    "Mozilla/5.0 (Linux; Android 13; SM-G991U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Mobile Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 16_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.3 Mobile/15E148 Safari/604.1"
]

# Character-level tokenization
tokenizer = {ch: i + 1 for i, ch in enumerate(set(''.join(user_agents)))}
tokenizer['<PAD>'] = 0

def tokenize(text, tokenizer):
    return [tokenizer[ch] for ch in text]

sequences = [tokenize(ua, tokenizer) for ua in user_agents]
max_seq_len = max(len(seq) for seq in sequences)

# Padding sequences
padded_sequences = [seq + [0] * (max_seq_len - len(seq)) for seq in sequences]
padded_sequences = torch.tensor(padded_sequences, dtype=torch.long)

# Convert to tensors and create labels (identity function for demonstration)
labels = padded_sequences.clone()

# Train/validation split
dataset = TensorDataset(padded_sequences, labels)
train_size = int(len(dataset) * 0.8)
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)

# Save the tokenizer
with open('tokenizer.pkl', 'wb') as f:
    pickle.dump(tokenizer, f)

# Parameters
latent_dim = 64  # Latent dimensionality for LSTM
bottleneck_dim = 20  # Target bottleneck vector length
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define the Encoder
class Encoder(nn.Module):
    def __init__(self, vocab_size, embedding_dim, latent_dim):
        super(Encoder, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, latent_dim, batch_first=True)

    def forward(self, x):
        embedded = self.embedding(x)
        outputs, (hidden, cell) = self.lstm(embedded)
        return outputs, hidden, cell

# Define the Attention Mechanism
class Attention(nn.Module):
    def __init__(self, latent_dim):
        super(Attention, self).__init__()
        self.attn = nn.Linear(latent_dim, latent_dim)
        self.v = nn.Parameter(torch.rand(latent_dim))

    def forward(self, encoder_outputs):
        energy = torch.tanh(self.attn(encoder_outputs))
        attention = torch.sum(energy * self.v, dim=2)
        return torch.softmax(attention, dim=1)

# Define the Decoder
class Decoder(nn.Module):
    def __init__(self, vocab_size, embedding_dim, latent_dim, bottleneck_dim):
        super(Decoder, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, latent_dim, batch_first=True)
        self.attention = Attention(latent_dim)
        self.fc = nn.Linear(latent_dim * 2, vocab_size)
        self.bottleneck = nn.Linear(latent_dim, bottleneck_dim)

    def forward(self, x, encoder_outputs, hidden, cell):
        embedded = self.embedding(x)
        outputs, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        attn_weights = self.attention(encoder_outputs)
        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)
        output = torch.cat((outputs.squeeze(1), context), dim=1)
        predictions = self.fc(output)
        bottleneck_vector = self.bottleneck(hidden.squeeze(0))
        return predictions, hidden, cell, bottleneck_vector

# Seq2Seq Model
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder):
        super(Seq2Seq, self).__init__()
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, src, trg):
        encoder_outputs, hidden, cell = self.encoder(src)
        outputs, bottleneck_vector = self.decoder(trg, encoder_outputs, hidden, cell)
        return outputs, bottleneck_vector

# Instantiate models
vocab_size = len(tokenizer)
embedding_dim = latent_dim

encoder = Encoder(vocab_size, embedding_dim, latent_dim).to(device)
decoder = Decoder(vocab_size, embedding_dim, latent_dim, bottleneck_dim).to(device)
model = Seq2Seq(encoder, decoder).to(device)

# Define the optimizer and loss function
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

# Early stopping parameters
patience = 3
best_val_loss = float('inf')
patience_counter = 0

# Training the model
num_epochs = 50
train_losses = []
val_losses = []

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs, _ = model(inputs, inputs)
        loss = criterion(outputs.view(-1, vocab_size), labels.view(-1))
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()

    train_loss = running_loss / len(train_loader)
    train_losses.append(train_loss)

    model.eval()
    val_loss = 0.0
    
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs, _ = model(inputs, inputs)
            loss = criterion(outputs.view(-1, vocab_size), labels.view(-1))
            val_loss += loss.item()
    
    val_loss /= len(val_loader)
    val_losses.append(val_loss)

    print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

    # Early stopping check
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        # Save the best model
        torch.save(model.state_dict(), 'best_seq2seq_model.pth')
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print("Early stopping triggered!")
            break

# Plot the training and validation loss
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

# Load the tokenizer
with open('tokenizer.pkl', 'rb') as f:
    loaded_tokenizer = pickle.load(f)

# Load the best model
loaded_model = Seq2Seq(encoder, decoder).to(device)
loaded_model.load_state_dict(torch.load('best_seq2seq_model.pth'))

# Get the bottleneck embeddings for a sample input
sample_input = torch.tensor([tokenize("Mozilla/5.0 (Linux; Android 13; SM-G991U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Mobile Safari/537.36", loaded_tokenizer) + [0] * (max_seq_len - 102)], dtype=torch.long).unsqueeze(0).to(device)
_, bottleneck_vector = loaded_model(sample_input, sample_input)

print("Bottleneck Embeddings for Sample Input:", bottleneck_vector.detach().cpu().numpy())
