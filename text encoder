import tensorflow as tf
from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention, Concatenate
from tensorflow.keras.models import Model
import numpy as np

# Sample user-agent string (example)
user_agents = ["Mozilla/5.0 (Linux; Android 13; SM-G991U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Mobile Safari/537.36"]

# Character-level tokenization
tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)
tokenizer.fit_on_texts(user_agents)
sequences = tokenizer.texts_to_sequences(user_agents)
max_seq_len = max(len(seq) for seq in sequences)

# Padding sequences
padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_seq_len, padding='post')

# Parameters
latent_dim = 64  # Latent dimensionality for LSTM
bottleneck_dim = 20  # Target bottleneck vector length

# Encoder model
encoder_inputs = Input(shape=(None,))
encoder_embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=latent_dim)(encoder_inputs)
encoder_lstm = LSTM(latent_dim, return_state=True)
encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)

# Attention mechanism
attention = Attention()
context_vector = attention([encoder_outputs, encoder_outputs])

# Bottleneck vector (fixed-length representation)
bottleneck = Dense(bottleneck_dim)(context_vector)

# Decoder model
decoder_inputs = Input(shape=(None,))
decoder_embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=latent_dim)(decoder_inputs)
decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])

# Attention applied to decoder outputs
attention_result = attention([decoder_outputs, context_vector])
decoder_concat_input = Concatenate(axis=-1)([decoder_outputs, attention_result])

decoder_dense = Dense(len(tokenizer.word_index) + 1, activation='softmax')
decoder_outputs = decoder_dense(decoder_concat_input)

# Seq2Seq model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

# Summary of the model
model.summary()

# Training (example, need proper training loop with actual labels)
# model.fit([padded_sequences, padded_sequences], np.expand_dims(padded_sequences, -1), epochs=10)

# Extract the bottleneck vector (e.g., during inference)
encoder_model = Model(encoder_inputs, bottleneck)
encoded_vector = encoder_model.predict(padded_sequences)

print("Encoded vector (bottleneck representation):", encoded_vector)
