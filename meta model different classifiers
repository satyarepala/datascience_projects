import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import precision_score, recall_score, confusion_matrix
from sklearn.model_selection import train_test_split

def evaluate_model(y_true, y_pred):
    # Calculate confusion matrix and extract TN, FP, FN, TP
    conf_matrix = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = conf_matrix.ravel()
    
    # Calculate precision and recall
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    
    # Print results
    print(f"Confusion Matrix:\n{conf_matrix}")
    print(f"True Positives (TP): {tp}")
    print(f"False Positives (FP): {fp}")
    print(f"True Negatives (TN): {tn}")
    print(f"False Negatives (FN): {fn}")
    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print("\n----------------------\n")


def train_models(non_fraud_data, fraud_data):
    # Labels for the data
    non_fraud_labels = np.zeros(non_fraud_data.shape[0])
    fraud_labels = np.ones(fraud_data.shape[0])

    # Combine data and labels
    X = np.vstack((non_fraud_data, fraud_data))
    y = np.hstack((non_fraud_labels, fraud_labels))

    # Split into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Define models with class_weight for LogisticRegression and RandomForest, and imbalance handling for XGBoost and LightGBM
    models = {
        "Logistic Regression": LogisticRegression(class_weight='balanced'),
        "Random Forest": RandomForestClassifier(class_weight='balanced', random_state=42),
        "XGBoost": XGBClassifier(scale_pos_weight=len(non_fraud_labels) / len(fraud_labels), random_state=42),
        "LightGBM": LGBMClassifier(scale_pos_weight=len(non_fraud_labels) / len(fraud_labels), random_state=42)
    }

    # Train and evaluate each model
    for model_name, model in models.items():
        print(f"Training {model_name}...")
        
        # Train the model
        model.fit(X_train, y_train)
        
        # Predict on the test set
        y_pred = model.predict(X_test)
        
        # Evaluate the model
        print(f"Results for {model_name}:")
        evaluate_model(y_test, y_pred)

# Example usage:
# Simulated data for non-fraud (5000 samples, 10 features) and fraud (500 samples, 10 features)
non_fraud_data = np.random.rand(5000, 10)
fraud_data = np.random.rand(500, 10)

train_models(non_fraud_data, fraud_data)
