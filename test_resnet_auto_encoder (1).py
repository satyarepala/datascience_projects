# -*- coding: utf-8 -*-
"""test_resnet_auto_encoder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-s0ywQj4OyAp1mUv4ZR0C8OTDeLHbjLJ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, random_split
import numpy as np
import matplotlib.pyplot as plt

# Define the CNNAutoencoder model
class CNNAutoencoder(nn.Module):
    def __init__(self, bottleneck_size=512):
        super(CNNAutoencoder, self).__init__()

        # Encoder: progressively reduce spatial dimensions
        self.encoder = nn.Sequential(
            nn.Conv1d(7, 16, kernel_size=7, stride=2, padding=3),  # Output: (16, 50)
            nn.ReLU(inplace=True),

            nn.Conv1d(16, 32, kernel_size=5, stride=2, padding=2),  # Output: (32, 26)
            nn.ReLU(inplace=True),

            nn.Conv1d(32, 64, kernel_size=3, stride=2, padding=1)  # Output: (64, 13)
        )

        # Add a fully connected layer to transform the output to a 1D vector
        self.flatten = nn.Flatten()
        self.fc = nn.Linear(64 * 13, bottleneck_size)  # 64 channels, 13 length

        # Decoder: progressively reconstruct spatial dimensions
        self.decoder = nn.Sequential(
            nn.Linear(bottleneck_size, 64 * 13),  # Transform back to the shape needed for decoding
            nn.Unflatten(1, (64, 13)),

            nn.ConvTranspose1d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: (32, 26)
            nn.ReLU(inplace=True),

            nn.ConvTranspose1d(32, 16, kernel_size=5, stride=2, padding=2, output_padding=1),  # Output: (16, 50)
            nn.ReLU(inplace=True),

            nn.ConvTranspose1d(16, 8, kernel_size=5, stride=2, padding=2, output_padding=1),   # Output: (8, 100)
            nn.ReLU(inplace=True),

            nn.ConvTranspose1d(8, 7, kernel_size=3, stride=1, padding=3, output_padding=0)    # Output: (7, 100)
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.flatten(x)
        x = self.fc(x)
        x = self.decoder(x)
        return x

    def get_bottleneck(self, x):
        x = self.encoder(x)
        x = self.flatten(x)
        x = self.fc(x)
        return x

# Initialize the model with desired bottleneck size
model = CNNAutoencoder(bottleneck_size=512)  # Change to 1024 if needed

# Weight initialization
def init_weights(m):
    if isinstance(m, nn.Conv1d) or isinstance(m, nn.ConvTranspose1d):
        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        if m.bias is not None:
            nn.init.constant_(m.bias, 0)

model.apply(init_weights)

# # Example input
input_tensor = torch.randn(1, 7, 100)  # Batch size of 1, 7 channels, 100 length
output_tensor = model(input_tensor)

print("Input shape:", input_tensor.shape)
print("Output shape:", output_tensor.shape)

# Create synthetic data for demonstration
num_samples = 10000
seq_length = 100
num_channels = 7

# Generating random data for training
x_data = np.random.randn(num_samples, num_channels, seq_length).astype(np.float32)
y_data = x_data  # For autoencoder, input and output are the same

# Convert to PyTorch tensors
x_data_tensor = torch.tensor(x_data)
y_data_tensor = torch.tensor(y_data)

# Split the data into training and validation sets (80% train, 20% val)
dataset = TensorDataset(x_data_tensor, y_data_tensor)
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

# Create DataLoader for training and validation
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# Define the loss function and optimizer
criterion = nn.MSELoss()  # Mean Squared Error Loss
optimizer = optim.Adam(model.parameters(), lr=1e-3)  # Adam optimizer


# Training loop
num_epochs = 20
patience = 3  # Number of epochs to wait for improvement
train_losses = []
val_losses = []

best_val_loss = float('inf')
patience_counter = 0

for epoch in range(num_epochs):
    model.train()
    running_train_loss = 0.0

    for inputs, targets in train_loader:
        optimizer.zero_grad()  # Zero the gradients

        outputs = model(inputs)  # Forward pass
        loss = criterion(outputs, targets)  # Compute loss

        loss.backward()  # Backward pass
        optimizer.step()  # Update weights

        running_train_loss += loss.item() * inputs.size(0)

    epoch_train_loss = running_train_loss / len(train_loader.dataset)
    train_losses.append(epoch_train_loss)

    # Validation loss
    model.eval()
    running_val_loss = 0.0

    with torch.no_grad():
        for inputs, targets in val_loader:
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            running_val_loss += loss.item() * inputs.size(0)

    epoch_val_loss = running_val_loss / len(val_loader.dataset)
    val_losses.append(epoch_val_loss)

    # Check for improvement
    if epoch_val_loss < best_val_loss:
        best_val_loss = epoch_val_loss
        patience_counter = 0  # Reset patience counter
        # Save the best model
        #torch.save(model.state_dict(), str(epoch) + '_cnnautoencoder.pth')
    else:
        patience_counter += 1

    # Early stopping
    if patience_counter >= patience:
        print("Early stopping triggered.")
        break


    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}')

# Save the trained model
torch.save(model.state_dict(), 'cnnautoencoder.pth')

# Plot training and validation losses
plt.figure(figsize=(10, 5))
plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')
plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Train and Validation Loss')
plt.legend()
plt.grid(True)
plt.show()

# Prepare a sample input tensor
model.load_state_dict(torch.load('cnnautoencoder.pth'))
sample_input = torch.randn(1, 7, 100)  # Batch size of 1, 7 channels, 100 length

# Get the bottleneck (encoded) representation
with torch.no_grad():
    bottleneck_representation = model.get_bottleneck(sample_input)

print("Sample Input Shape:", sample_input.shape)
print("Bottleneck Representation Shape:", bottleneck_representation.shape)

bottleneck_representation

